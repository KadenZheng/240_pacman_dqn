# Pac-Man Reinforcement Learning with Transition Function Variations

This project investigates how modifying the transition function in an RL environment affects an agent's generalization and adaptability. It uses a Pac-Man environment to test how a Deep Q-Network (DQN) agent performs when ghost behavior (transition function) changes.

## Hypothesis

An RL agent trained under a fixed transition function will experience performance degradation when tested under varying transition conditions. Domain randomization during training may improve adaptability.

## Setup and Requirements

### Prerequisites

-   Python 3.9+
-   TensorFlow 2.x
-   NumPy
-   Matplotlib

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/pacman-rl-transitions.git
cd pacman-rl-transitions

# Install dependencies
pip install tensorflow numpy matplotlib
```

## Project Components

### Core Files

-   `pacman.py`: Main game environment
-   `DQN.py`: Deep Q-Network implementation
-   `pacmanDQN_Agents.py`: Pac-Man DQN agent
-   `ghostAgents.py`: Different ghost behavior implementations
-   `heuristicAgent.py`: Heuristic baseline agent

### Transition Function Variations

The project implements different ghost behaviors:

1. **DirectionalGhost**: Standard ghost that prefers to chase Pac-Man
2. **HighlyRandomGhost**: Ghost with increased random movement
3. **AggressiveGhost**: Ghost that aggressively chases Pac-Man
4. **PathfindingGhost**: Ghost using probabilistic A\* pathfinding

The `TransitionFunctionController` in `ghostAgents.py` manages transitions between different ghost behaviors.

### Training Modes

-   **Fixed**: Train with standard ghost behavior (DirectionalGhost)
-   **Progressive**: Gradually transition between ghost behaviors during training
-   **Random**: Randomly select ghost behavior for each episode
-   **Domain Randomization**: Randomize ghost parameters and behaviors during training

## Running Experiments

### Training

Run the training script to train agents with different transition function modes:

```bash
./run_training_with_saving.sh
```

This will:

1. Train a baseline DQN with fixed transition function
2. Train a DQN with progressive transition function shifts
3. Train a DQN with domain randomization
4. Train a DQN with random transition function shifts
5. Generate training metrics plots
6. Save model progress in the saves folder for later evalutation

### Benchmarking

Compare different agents' performance:

```bash
./run_benchmarks.sh
```

This will:

1. Test the heuristic baseline agent
2. Test DQN agents trained with different transition modes
3. Test all agents against unseen ghost behaviors (highly aggressive and highly random)
4. Generate benchmark comparison plots

## Analyzing Results

The `plot_metrics.py` script visualizes training metrics, and `analyze_benchmarks.py` analyzes benchmark results.

```bash
python analyze_benchmarks.py
```

Key metrics include:

-   Final score
-   Win rate
-   Survival time (steps per episode)
-   Adaptability to unseen ghost behaviors

## Visualizing Game Play

To watch a trained agent play:

```bash
python pacman.py -p PacmanDQN -a load_file=saves/model-domain_random_best.pkl -l mediumClassic
```

## Project Structure

```
.
├── README.md
├── pacman.py               # Main game environment
├── DQN.py                  # DQN implementation
├── pacmanDQN_Agents.py     # Pac-Man DQN agent
├── ghostAgents.py          # Ghost agents with different behaviors
├── heuristicAgent.py       # Heuristic baseline agent
├── run_training.sh         # Script to run training experiments
├── run_benchmarks.sh       # Script to run benchmarks
├── plot_metrics.py         # Script to plot training metrics
├── analyze_benchmarks.py   # Script to analyze benchmark results
├── layouts/                # Maze layouts
├── logs/                   # Training logs and metrics
├── saves/                  # Saved agent models
└── plots/                  # Generated plots
```

## Acknowledgments

This project is based on:

-   Berkeley AI Pacman projects
-   Deep Q-Network implementation by Tejas Kulkarni

## References

1. Ada, S. E. et al. (2019). "Generalization in transfer learning: robust control of robot locomotion." Robotica 40: 3811-3836.
2. Peng, X. B. et al. (2017). "Sim-to-Real Transfer of Robotic Control with Dynamics Randomization." IEEE International Conference on Robotics and Automation.
3. Ball, P. J. et al. (2021). "Augmented World Models Facilitate Zero-Shot Dynamics Generalization From a Single Offline Environment." International Conference on Machine Learning.
4. Tiboni, G. et al. (2023). "Domain Randomization via Entropy Maximization." ArXiv abs/2311.01885.
